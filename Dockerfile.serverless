# Dockerfile.serverless - Optimized for RunPod Serverless
FROM nvidia/cuda:12.8.1-runtime-ubuntu22.04

# Environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    DEBIAN_FRONTEND=noninteractive \
    CUDA_VISIBLE_DEVICES=0 \
    HF_HOME=/app/hf_cache \
    HF_DATASETS_CACHE=/app/hf_cache \
    TRANSFORMERS_CACHE=/app/hf_cache

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    libsndfile1 \
    ffmpeg \
    python3 \
    python3-pip \
    python3-dev \
    git \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Convenience: symlink python
RUN ln -s /usr/bin/python3 /usr/bin/python

# Workdir
WORKDIR /app

# Copy requirements file first (better Docker cache usage)
COPY requirements-serverless.txt .

# Upgrade pip
RUN pip3 install --no-cache-dir --upgrade pip

# Install PyTorch + CUDA 12.1 wheels
RUN pip3 install --no-cache-dir torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Install other Python dependencies
RUN pip3 install --no-cache-dir -r requirements-serverless.txt

# Install s3tokenizer separately without deps (avoid conflicts)
RUN pip3 install --no-cache-dir --no-deps s3tokenizer

# Copy application code
COPY . .

# Create runtime dirs with correct permissions
RUN mkdir -p voices reference_audio outputs logs hf_cache && \
    chmod 755 voices reference_audio outputs logs hf_cache

# Ensure handler is executable
RUN chmod +x handler.py

# DISABLED: Pre-download model (causing worker timeout)
# Model will be downloaded on first request instead (1-2 minutes for first call)
# This allows workers to start quickly (30-60 seconds)
RUN echo "Model will be downloaded at runtime for faster worker startup"

# Entry point for RunPod serverless
CMD ["python", "handler.py"]